{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e48a6a8-a06c-474c-b3c1-8fa881c00146",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.inp_nodes = input_nodes\n",
    "        self.hid_nodes = hidden_nodes\n",
    "        self.out_nodes = output_nodes\n",
    "        self.lr = learning_rate\n",
    "        #We are following standard normal distribution to distribute and generate the normal weights randomly from the mean 0.0 to root_over(hidden_nodes,-0.5)\n",
    "        self.input_weights = np.random.normal(0.0,pow(self.hid_nodes,-0.5),(self.hid_nodes,self.inp_nodes)) #The dimension of the weight matrix between the input\n",
    "                                                                                                            # and the hidden layer should be (rowXcolumn) = (hid_nodes*inp_nodes)\n",
    "                                                                                                            # because the column and the row should be the same size while multiplication.\n",
    "        self.output_weights = np.random.normal(0.0,pow(self.out_nodes,-0.5),(self.out_nodes,self.hid_nodes)) #Weights connecting theprevious layer with the output layer before\n",
    "                                                                                                             #before the activation function that pushes out the final output.\n",
    "    def sigmoid(self,inputs):\n",
    "        return 1 / (1 + np.exp(-inputs))\n",
    "    \n",
    "\n",
    "    def train (self,input_list,target_list):\n",
    "        inputs = np.array(input_list,ndmin = 2).T\n",
    "        targets = np.array(target_list, ndmin = 2).T\n",
    "        hidden_input = np.dot(self.input_weights,inputs)\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output_layer_input = np.dot(self.output_weights, hidden_output)\n",
    "        self.final_outputs = self.sigmoid(output_layer_input)\n",
    "        output_errors = targets - self.final_outputs\n",
    "        hidden_errors = np.dot(self.output_weights.T,output_errors)\n",
    "        #We need use the derived formula from our derivative which we formulated from the mean erros square function. The derivative is the slope of the function hence\n",
    "        #minimization\n",
    "        self.input_weights += self.lr*hidden_errors*self.sigmoid(hidden_output)*(1.0-self.sigmoid(hidden_output))*np.transpose(inputs)\n",
    "        self.output_weights += self.lr*output_errors*self.sigmoid(self.final_outputs)*(1.0-self.sigmoid(self.final_outputs))*np.transpose(hidden_output)\n",
    "        pass\n",
    "    def query(self,input_list):\n",
    "        inputs = np.array(input_list,ndmin =2).T\n",
    "        hidden_inps = np.dot(self.input_weights,inputs)\n",
    "        hidden_outs = self.sigmoid(hidden_inps) # When exiting the hidden layer the hidden_inputs go through an activation function\n",
    "        output_layer_inputs = np.dot(self.output_weights,hidden_outs) # When entering the output layer the hidden_outputs are multiplied and summed with weighted links from\n",
    "                                                               # the previous layer to the output_layer\n",
    "        outputs = self.sigmoid(output_layer_inputs) # When exiting the final outputlayer the output layer inputs go throw the activation function again. \n",
    "                                                    # This is simply the forward pass.\n",
    "        return outputs\n",
    "\n",
    "    def performance(self,test_data_list): \n",
    "        for record in test_data_list:\n",
    "            # split the record by the ',' commas\n",
    "            all_values = record.split(',')\n",
    "            # correct answer is first value\n",
    "            correct_label = int(all_values[0])\n",
    "            # scale and shift the inputs\n",
    "            inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "            # query the network\n",
    "            outputs = n.query(inputs)\n",
    "            # the index of the highest value corresponds to the label\n",
    "            label = numpy.argmax(outputs)\n",
    "            # append correct or incorrect to list\n",
    "            if (label == correct_label):\n",
    "            # network's answer matches correct answer, add 1 to\n",
    "            #scorecard\n",
    "                scorecard.append(1)\n",
    "            else:\n",
    "            # network's answer doesn't match correct answer, add 0 to\n",
    "            #scorecard\n",
    "                scorecard.append(0)\n",
    "        \n",
    "            # calculate the performance score, the fraction of correct answers\n",
    "            scorecard_array = numpy.asarray(scorecard)\n",
    "            performance = scorecard_array.sum() / scorecard_array.size\n",
    "            return performance\n",
    "            \n",
    "    def cost_function(self,target):   \n",
    "        return 0.5*sum((self.final_outputs-target)**2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fe294-e2b7-4634-a171-54e6617e8f7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "import matplotlib.pyplot as plot \n",
    "home = expanduser(\"~\")\n",
    "home = home.replace(\"\\\\\",\"/\")+\"/\"\n",
    "data = open(home+\"Downloads/Python_Artificial/MNIST_dataset/mnist_train.csv\")\n",
    "data=data.readlines()\n",
    "all_values = data[0].split(\",\")\n",
    "image_array = np.asfarray(all_values[1:]).reshape(28,28)\n",
    "plot.imshow(image_array,cmap = 'grey',interpolation= None)\n",
    "# Weight initilization formula - > So\n",
    "# if each node has 3 links into it, the initial weights should be in the range 1/(√3) = 0.577. If each\n",
    "# node has 100 incoming links, the weights should be in the range 1/(√100) = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b4c0c-5d9b-480c-bd4d-afe3dc08664b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#scaling the raw input from 0-255 to 0.01 to 1.0 and perform the actual training of the dataset\n",
    "input_nodes = 784\n",
    "hidden_nodes =100\n",
    "output_nodes =10\n",
    "learning_rate =0.02 #It can be anything between 0.01 to 0.5, Finding sweet spot takes time.\n",
    "n = NeuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)\n",
    "#n.query([1, 0.5, -1.5]) We are simply observing a neural net outputs from initiated random weights and inputs. \n",
    "#The query is exactly the same as a neural net forward pass\n",
    "#Training of the actual network\n",
    "training_data = data\n",
    "cost =[]\n",
    "for i in range(10000):\n",
    "        print(i)\n",
    "        for record in training_data:\n",
    "            all_values = record.split(\",\")\n",
    "            inputs = (np.asfarray(all_values[1:])/(255.00*0.99))+0.01\n",
    "            if len(inputs)<input_nodes:\n",
    "                pass\n",
    "            else:\n",
    "                targets = np.zeros(output_nodes)+0.1\n",
    "                targets[int(all_values[0])]=0.99\n",
    "                n.train(inputs,targets)\n",
    "                c = n.cost_function(targets)\n",
    "                cost.append(c)\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d9cb4-0222-4b59-b467-5aa5edc890c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = open(home+\"Downloads/Python_Artificial/MNIST_dataset/mnist_test_10.csv\")\n",
    "test_data= test_data.readlines()\n",
    "test_values = test_data[0].split(\",\")\n",
    "img_array = np.asfarray(test_values[1:]).reshape(28,28)\n",
    "plot.imshow(img_array,cmap=\"grey\",interpolation = None)\n",
    "test_data = (np.asfarray(test_values[1:])/(255.00*0.99)+ 0.01)\n",
    "output = n.query(test_data)\n",
    "print(np.argmax(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbc594-b265-488f-a382-7d5cf12fcb22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reversing a neural net to understand its mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7715d3-152b-4e04-a3bb-5955be965cb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot.grid()\n",
    "plot.plot(range(10000),cost)\n",
    "\n",
    "plot.title('Cost Function')\n",
    "plot.xlabel('Training Iterations')\n",
    "plot.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7220b5-284f-47ea-a878-a1c7d49a5d5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adagrad Optimizer\n",
    "class NeuralNetworkWithAdagrad:\n",
    "\n",
    "            def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "                #Initiallizing the nodes\n",
    "                self.inp_nodes = input_nodes\n",
    "                self.hid_nodes = hidden_nodes\n",
    "                self.out_nodes = output_nodes\n",
    "                self.lr = learning_rate\n",
    "            \n",
    "                #Initializing the weights and biases\n",
    "                self.inptohid_weights = np.random.normal(0.0,pow(self.hid_nodes,-0.5),(self.hid_nodes,self.inp_nodes))\n",
    "                self.bias1 = np.zeros((1,hid_nodes))\n",
    "                self.hidtoout_weights = np.random.normal(0.0,pow(self.out_nodes,-0.5),(self.out_nodes,self.hid_nodes))\n",
    "                self.bias2 = np.zeros((1,out_nodes)) # Tuple \n",
    "            \n",
    "                #Initializing Adagrad Parameters \n",
    "                self.grad_inputtohid_W2 = np.zeros_like(self.inptohid_weights)\n",
    "                self.grad_bias1 = np.zeros_like(self.bias1)\n",
    "                self.grad_hidtoout_W1 = np.zeros_like(self.hidtoout_weights)\n",
    "                self.grad_bias2 = np.zeros_like(self.bias2)\n",
    "            \n",
    "            def train():\n",
    "               pass\n",
    "               \n",
    "            def forward_pass(self,X):\n",
    "               x = np.array(X,ndimn = 2).T\n",
    "               self.z1 = np.dot(self.inptohid_weights,x)+self.bias1\n",
    "               self.a1 = self.relU(self.z1)\n",
    "               self.z2 = np.dot(self.hidtoout_weights,self.a1) + self.bias2\n",
    "               self._pred_y = self.softmax(self.z2)\n",
    "               return self._pred_y\n",
    "               \n",
    "            def backward_pass(Y,X):\n",
    "            #In back propagation we are trying to find the gradient of the loss function with respect to the weights and biases\n",
    "            #therefore when we are applying derivative rules over the loss function we come up with an equation that shows the \n",
    "            #direct relation with the loss respect to the weights. It means increasing the a unit in the parameters (weights, bias) can \n",
    "            #makes a significant change in the loss function. It might increase it or decrease it. In gradient optimization we simply\n",
    "            #substract the gradient from the weight because a positive gradient means a larger increase and a negative gradient can mean \n",
    "            #denote a potential decrease. Negative gradients are added and positve gradients are substracted from the weights which\n",
    "            #comes up with the desired parameters which we want for our model. \n",
    "               true_y = Y\n",
    "               delta2 = self._pred_y - true_y # (Cross Entropy Loss) Derivate related to the loss function and W2 or dl/dW2 is\n",
    "                                              #(a2 - y)*a1 (gradients). [(y_hat is a2) from the forward pass]. \n",
    "                                              #Because according to the chainrule for the\n",
    "                                              #second set of weights related to the outputlayer is dL/dw2 = dl/da2*da2/dz2*dz2/dw2\n",
    "                                              #where dl/dw2 =\n",
    "                                              #(dl/da2*da2/dz2)*d(w2*a1)/dw2 = d(sum(y_hat-log(a2))/da2*d(softmax_derivative(z2))/dz2*d(W2*a1)/dW2\n",
    "                                              #=(-sum(y*(1/a2))*a2(1-a2))*a1 [Because ya2 is 0 always for the incorrect class. Y is one hot encoded vector with 0 and 1\n",
    "                                              #hence we take the first part of softmax derivative which is a2(1-a2)]\n",
    "                                              #=(a2-y)*a1 \n",
    "               dW2 = np.dot(delta2,self.a1.T) #From the formula.\n",
    "               db2 = np.sum(delta2,axis = 0, keepdims = True)\n",
    "                \n",
    "               #Let us calculate dW1 for the first set of weights (from the input_layer to the hidden layer)\n",
    "               #Since we already have the error gradients which is delta2, we will distribute \n",
    "               #delta2 inside the hidden layer back\n",
    "               #to the inputlayer. The formula is \n",
    "               #da1/dw1 = da1/dz1*dz1/dw1*(W2*delta2)\n",
    "               #d(relu(z1))/dz1*d(w1*x)/dw1*(W2*delta2)\n",
    "               #relu'(z1)*(W2*delta2)*x\n",
    "               \n",
    "               delta1 = np.dot(self.hidtoout_weights.T,delta2)*self.relu_derivative(z1)\n",
    "               dW1 = np.dot(delta1,X.T)\n",
    "               db1 = np.sum(delta1,axis = 0)\n",
    "               return dW1,db1,dW2,db2\n",
    "            \n",
    "            def relU(self,X):\n",
    "               return np.maximum(0,X)\n",
    "               \n",
    "            def relU_derivative(self,X):\n",
    "               return np.where(x>0, 1, 0)\n",
    "               \n",
    "            def softmax(self,z2):\n",
    "               ex_y = np.exp(z2) / np.sum(np.exp(z2),axis = 1, keepdims = True)\n",
    "               return ex_y/ex_y.sum(axis = 0)\n",
    "               \n",
    "            def cost_function(self,y_true):\n",
    "               loss = -np.sum(y_true * np.log(self._pred_y))\n",
    "               return loss\n",
    "               \n",
    "            def Adagrad_Update(self,dW1,db1,dW2,db2):\n",
    "               \n",
    "                self.grad_hidtoout_W1 += dW1 ** 2\n",
    "                self.grad_bias1 += db1 ** 2\n",
    "                self.grad_inputtohid_W2 += dW2 ** 2\n",
    "                self.grad_bias2 += db2 ** 2\n",
    "                \n",
    "                self.hidtoout_weights -= learning_rate * dW1 / (np.sqrt(self.grad_hidtoout_W1) + self.eps)\n",
    "                self.bias1 -= learning_rate * db1 / (np.sqrt(self.grad_bias1) + self.eps)\n",
    "                self.inptohid_weights -= learning_rate * dW2 / (np.sqrt(self.grad_inputtohid_W2) + self.eps)\n",
    "                self.bias2 -= learning_rate * db2 / (np.sqrt(self.grad_bias2) + self.eps)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d875e5d9-e2b4-4a42-b9b0-830f587cabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam Optimizer\n",
    "class NeuralNetworkwithAdamOptimizer:\n",
    "\n",
    "    def __init__(self,input_nodes,output_nodes,hidden_nodes,learning_rate):\n",
    "        self.inp_nodes = input_nodes\n",
    "        self.hid_nodes = hidden_nodes\n",
    "        self.out_nodes = output_nodes\n",
    "\n",
    "        self.W1 = np.random.normal(0.0,pow(self.hid_nodes,-0.5),(self.hid_nodes,self.inp_nodes))\n",
    "        self.b1 = np.zeros((1,hid_nodes))\n",
    "        self.W2 = np.random.normal(0.0,pow(self.out_nodes,-0.5),(self.out_nodes,self.hid_nodes))\n",
    "        self.b2 =  np.zeros((1,out_nodes))\n",
    "\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.mW1 = np.zeros_like(self.W1)\n",
    "        self.vW1 = np.zeros_like(self.W1)\n",
    "        self.mb1 = np.zeros_like(self.b1)\n",
    "        self.vb1 = np.zeros_like(self.b1)\n",
    "        self.mW2 = np.zeros_like(self.W2)\n",
    "        self.vW2 = np.zeros_like(self.W2)\n",
    "        self.mb2 = np.zeros_like(self.b2)\n",
    "        self.vb2 = np.zeros_like(self.b2)\n",
    "        self.t = 0\n",
    "        \n",
    "        def train(self):\n",
    "            pass\n",
    "        \n",
    "        def relu(self,X):\n",
    "            return np.maximum(0,X)\n",
    "        \n",
    "        def relu_der(self,X):\n",
    "            return np.where(x>0, 1,0)\n",
    "        \n",
    "        def softmax(self,X):\n",
    "            e_X = np.exp(X) / np.sum(np.exp(X),axis = 1, keepdims = True)\n",
    "            return e_X/e_X.sum(axis = 0)\n",
    "        \n",
    "        def softmax_der(self,y_pred):\n",
    "            softmax_output = np.expand_dims(probs, axis=-1)\n",
    "            return np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T)\n",
    "        \n",
    "        def cost_function(self,y_true,y_pred):\n",
    "            loss = -np.sum(y_true * np.log(y_pred))\n",
    "            return loss\n",
    "        \n",
    "        def forward_pass(self,X):\n",
    "            X = np.array(X,ndmin =2).T\n",
    "            z1 = np.dot(self.W1,X)+self.b1\n",
    "            a1 = self.relu(z1)\n",
    "            z2 = np.dot(self.W2,a1)+self.b2\n",
    "            y_pred = self.softmax(z2)\n",
    "            \n",
    "            return y_pred\n",
    "                             \n",
    "        def backward_propagation(self,y_true,y_pred,X):\n",
    "            delta2 = y_pred-y_true #Cross entropy categorical loss derivative\n",
    "            dW2 = np.dot(delta1,a1.T) #1. dL/dW2 = (dL/dy_pred)*(dy_pred/dz2)*(dz2/da1)\n",
    "            db2 = np.sum(delta1,axis = 0, keepdims =True)\n",
    "            delta1 = np.dot(delta2,W2.T)*self.relu_der(z1)\n",
    "            dW1 = np.dot(delta1,X.T) #2. dL/dW1 = (da1/dz1)*(dz1/dW1)*delta2*W2 [Back propagate the errors]\n",
    "            db1 = np.sum(delta1,axis =0) #The obtained gradient formula is applied on both of the junctions. Inp to Hid layer\n",
    "                                      #and Hid to Out Layer. \n",
    "            return dW1,db1,dW2,db2\n",
    "        \n",
    "        def Adam_Optimizer(self,dW1,db1,dW2,db2):\n",
    "            self.t += 1\n",
    "            self.mW1 = self.beta1 * self.mW1 + (1 - self.beta1) * dW1\n",
    "            self.vW1 = self.beta2 * self.vW1 + (1 - self.beta2) * (dW1 ** 2)\n",
    "            self.mb1 = self.beta1 * self.mb1 + (1 - self.beta1) * db1\n",
    "            self.vb1 = self.beta2 * self.vb1 + (1 - self.beta2) * (db1 ** 2)\n",
    "            mW1_hat = self.mW1 / (1 - self.beta1 ** self.t)\n",
    "            vW1_hat = self.vW1 / (1 - self.beta2 ** self.t)\n",
    "            mb1_hat = self.mb1 / (1 - self.beta1 ** self.t)\n",
    "            vb1_hat = self.vb1 / (1 - self.beta2 ** self.t)\n",
    "            self.W1 -= learning_rate * mW1_hat / (np.sqrt(vW1_hat) + self.epsilon)\n",
    "            self.b1 -= learning_rate * mb1_hat / (np.sqrt(vb1_hat) + self.epsilon)\n",
    "            \n",
    "            self.mW2 = self.beta1 * self.mW2 + (1 - self.beta1) * dW2\n",
    "            self.vW2 = self.beta2 * self.vW2 + (1 - self.beta2) * (dW2 ** 2)\n",
    "            self.mb2 = self.beta1 * self.mb2 + (1 - self.beta1) * db2\n",
    "            self.vb2 = self.beta2 * self.vb2 + (1 - self.beta2) * (db2 ** 2)\n",
    "            mW2_hat = self.mW2 / (1 - self.beta1 ** self.t)\n",
    "            vW2_hat = self.vW2 / (1 - self.beta2 ** self.t)\n",
    "            mb2_hat = self.mb2 / (1 - self.beta1 ** self.t)\n",
    "            vb2_hat = self.vb2 / (1 - self.beta2 ** self.t)\n",
    "            self.W2 -= learning_rate * mW2_hat / (np.sqrt(vW2_hat) + self.epsilon)\n",
    "            self.b2 -= learning_rate *  mb2_hat / (np.sqrt(vb2_hat) + self.epsilon)\n",
    "            \n",
    "            return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec24a45-b445-4c5c-b5bf-276cd0d4b91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
